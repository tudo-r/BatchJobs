#!/bin/bash
## Job Resource Interface Definition
##
## ntasks [integer(1)]:       Number of required tasks,
##                            Set larger than 1 if you want to further parallelize
##                            with MPI within your job.
## ncpus [integer(1)]:        Number of required cpus per task,
##                            Set larger than 1 if you want to further parallelize
##                            with multicore/parallel within each task.
## walltime [integer(1)]:     Walltime for this job, in minutes.
##                            Must be at least 1 minute.
## memory   [integer(1)]:     Memory in megabytes for each cpu.
##                            Must be at least 100 (when I tried lower values my
##                            jobs did not start at all).
##
## Default resources can be set in your .BatchJobs.R by defining the variable
## 'default.resources' as a named list.

## For MPI use something like
## <%
## cmd = "R CMD BATCH --no-save --no-restore"
## if(ntasks > 1L) 
##   cmd = paste("mpirun -np 1", cmd)
## 
## -%>

#SBATCH --job-name=<%= job.name %>
## direct streams to our logfile
#SBATCH --output=<%= log.file %>
#SBATCH --error=<%= log.file %>
#SBATCH --time=<%= resources$walltime %>
#SBATCH --ntasks=<%= resources$ntasks %>
#SBATCH --cpus-per-task=<%= resources$ncpus %>
#SBATCH --mem-per-cpu=<%= resources$memory %>
## remove this line if your cluster does not support arrayjobs
#SBATCH --array=1-<%= arrayjobs %> 
  
## Initialize work environment like
## source /etc/profile
## module add ...

## Run R:
## we merge R output with stdout from SLURM, which gets then logged via --output option
R CMD BATCH --no-save --no-restore "<%= rscript %>" /dev/stdout
